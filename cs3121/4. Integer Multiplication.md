# Fast Large Integer Multiplication

## Generalizing Karatsuba's Algorithm

Slice the input numbers $A$, $B$ into $n + 1$ many slices. For simplicity, let $A$, $B$ have $(n + 1)k$ bits ($k$ can be arbitrarily large but $n$ is fixed).

Slice $A$, $B$ into $n + 1$ pieces each:

$$
\begin{array}{lcl}
  A & = & A_n 2^{kn} + A_{n - 1} 2^{k(n - 1)} + \dots + A_0 \\
  B & = & B_n 2^{kn} + B_{n - 1} 2^{k(n - 1)} + \dots + B_0 \\
\end{array}
$$

Form the naturally corresponding polynomials:

$$
\begin{array}{lcl}
  P_A(x) & = & A_n x^n + A_{n - 1} x^{n - 1} + \dots + A_0 \\
  P_B(x) & = & B_n x^n + B_{n - 1} x^{n - 1} + \dots + B_0 \\
\end{array}
$$

We have $A = P_A (2^k)$, $B = P_B (2^k)$ and $AB = P_A(2^k) P_B(2^k) = (P_A(x) \cdot P_B(x)) \|_{x = 2^k}$.

We adopt the following strategy:

- First figure out how to multiply polynomials fast to obtain $P_C(x) = P_A(x) \cdot P_B(x)$
- Then evaluate $P_C(2^k)$

We need to find the coefficients $C_j = \sum_{i + k = j} A_i B_k$ without performing $(p + 1)^2$ many multiplications necessary to get all products of the form $A_i B_k$.

### Linear Convolution Of Sequences

If you have two sequences $\vec{A} = (A_0, A_1, \dots, A_{p - 1}, A_p)$ and $\vec{A} = (B_0, B_1, \dots, B_{m - 1}, B_m)$, and if you form the two corresponding polynomials

$$
\begin{array}{lcl}
  P_A(x) & = & A_p x^p + A_{p - 1} x^{p - 1} + \dots + A_1 x + A_0 \\
  P_B(x) & = & B_m x^m + B_{m - 1} x^{m - 1} + \dots + B_1 x + B_0 \\
\end{array}
$$

and if you multiply these two polynomials to obtain their product

$$
P_A(x) \cdot P_B(x) = \sum_{j = 0}^{m + p} \left( \sum_{i + k = j} A_i B_k \right) x^j = \sum_{j = 0}^{p + m} C_j x^j
$$

then the sequence $\vec{C} = (C_0, C_1, \dots, C_{p + m})$ of the coefficients of the product polynomial, with these coefficients given by

$$
C_j = \sum_{i + k = j} A_i B_k, \quad \text{for} \quad 0 \le j \le p + m
$$

is called the **linear convolution** of sequences $\vec{A}$ and $\vec{B}$ and is denoted by $\vec{C} = \vec{A} \ast \vec{B}$.

### Vandermonde Matrix

Every polynomial $P_A(x)$ of degree $p$ is uniquely determined by its values at any $p + 1$ distinct input values $x_0, x_1, \dots, x_p$:

$$
P_A(x) \leftrightarrow \{ (x_0, P_A(x_0)), (x_1, P_A(x_1)), \dots, (x_p, P_A(x_p)) \}
$$

For $P_A(x) = A_px^p + A_{p - 1}x^{p - 1} + \dots + A_0$, these values can be obtained via a matrix multiplication:

$$
\begin{pmatrix}
  1 & x_0 & x_0^2 & \dots & x_0^p \\
  1 & x_1 & x_1^2 & \dots & x_1^p \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  1 & x_p & x_p^2 & \dots & x_p^p
\end{pmatrix}
\begin{pmatrix}
  A_0 \\
  A_1 \\
  \vdots \\
  A_p
\end{pmatrix}
= 
\begin{pmatrix}
  P_A(x_0) \\
  P_A(x_1) \\
  \vdots \\
  P_A(x_p)
\end{pmatrix}
\qquad (1)
$$

It can be shown that if $x_i$ are all distinct then this matrix is invertible. Such a matrix is called the **Vandermonde Matrix**.

If all $x_i$ are distinct, given any values $P_A(x_0), P_A(x_1), \dots, P_A(x_p)$ the coefficients $A_0, A_1, \dots, A_p$ of the polynomial $P_A(x)$ are uniquely determined:

$$
\begin{pmatrix}
  A_0 \\
  A_1 \\
  \vdots \\
  A_p
\end{pmatrix}
\begin{pmatrix}
  1 & x_0 & x_0^2 & \dots & x_0^p \\
  1 & x_1 & x_1^2 & \dots & x_1^p \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  1 & x_p & x_p^2 & \dots & x_p^p
\end{pmatrix}^{-1}
= 
\begin{pmatrix}
  P_A(x_0) \\
  P_A(x_1) \\
  \vdots \\
  P_A(x_p)
\end{pmatrix}
\qquad (2)
$$

Equations (1) and (2) show how we can commute between:

1. A representation of a polynomial $P_A(x)$ via its coefficients $A_p, A_{p - 1}, \dots, A_0$, i.e. $P_A(x) = A_p x^p + \dots + A_1 x + A_0$
2. A representation of a polynomial $P_A(x)$ via its values 
$$
P_A(x) \leftrightarrow \{ (x_0, P_A(x_0)), (x_1, P_A(x_1)), \dots, (x_p, P_A(x_p)) \}
$$

If we fix the inputs $x_0, x_1, \dots, x_p$ then commuting between a representation of a polynomial $P_A(x)$ via its coefficients and a representation via its values at these points is done via the following two matrix multiplications, with matrices made up from **constants**:

$$
\begin{pmatrix}
  P_A(x_0) \\
  P_A(x_1) \\
  \vdots \\
  P_A(x_p)
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_0 & x_0^2 & \dots & x_0^p \\
  1 & x_1 & x_1^2 & \dots & x_1^p \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  1 & x_p & x_p^2 & \dots & x_p^p
\end{pmatrix}
\begin{pmatrix}
  A_0 \\
  A_1 \\
  \vdots \\
  A_p
\end{pmatrix}
$$

$$
\begin{pmatrix}
  A_0 \\
  A_1 \\
  \vdots \\
  A_p
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_0 & x_0^2 & \dots & x_0^p \\
  1 & x_1 & x_1^2 & \dots & x_1^p \\
  \vdots & \vdots & \vdots & \vdots & \vdots \\
  1 & x_p & x_p^2 & \dots & x_p^p
\end{pmatrix}^{-1}
\begin{pmatrix}
  P_A(x_0) \\
  P_A(x_1) \\
  \vdots \\
  P_A(x_p)
\end{pmatrix}
$$

Thus, for fixed input values $x_0, \dots, x_p$, this switch between the two kinds of
representations is done in **linear time**.

### Strategy

1. Given two polynomials of degree at most $p$, convert them into value representation at $2p + 1$ distinct points $x_0, x_1, \dots, x_{2p}$:

$$
\begin{array}{lcl}
  P_A(x) & \leftrightarrow & \{ (x_0, P_A(x_0)), (x_1, P_A(x_1)), \dots, (x_{2p}, P_A(x_{2p})) \} \\
  P_B(x) & \leftrightarrow & \{ (x_0, P_B(x_0)), (x_1, P_B(x_1)), \dots, (x_{2p}, P_B(x_{2p})) \} \\
\end{array}
$$

2. Multiply these two polynomials point-wise, using $2p + 1$ multiplications only:

$$
P_A(x) P_B(x) \leftrightarrow \{ (x_0, \underbrace{P_A(x_0) P_B(x_0)}_{P_C(x_0)}), (x_1, \underbrace{P_A(x_1) P_B(x_1)}_{P_C(x_1)}), \dots (x_{2p}, \underbrace{P_A(x_{2p}) P_B(x_{2p})}_{P_C(x_{2p})}) \}
$$

3. Convert such value representation of $P_C(x) = P_A(x) P_B(x)$ back to coefficient form

$$
P_C(x) = C_{2p} x^{2p} + C_{2p - 1} x^{2p - 1} + \dots + C_1 x + C_0
$$

- What values should we choose for $x_0, x_1, \dots, x_{2p}$? 
- Use $2p + 1$ smallest possible integer values $\{ -p, -(p - 1), \dots, -1, 0, \dots, p - 1, p \}$
- We find the values $P_A(m)$ and $P_B(m)$ for all $m$ such that $-p \le m \le p$
- Multiplication of a large number with $k$ bits by a constant integer $d$ can be done in time linear in $k$ because it is reducible to $d - 1$ additions: $d \cdot A = \underbrace{A + A + \dots + A}_{d}$
- Thus, all the values
$$
\begin{array}{lclcl}
  P_A(m) & = & A_p m^p + A_{p - 1} m^{p - 1} + \dots + A_0 & : & -p \le m \le p \\
  P_B(m) & = & B_p m^p + B_{p - 1} m^{p - 1} + \dots + B_0 & : & -p \le m \le p
\end{array}
$$
can be found in time linear in the number of bits of the input numbers

- We now perform $2p + 1$ **multiplications of large numbers** to obtain
$$
P_A(-p) P_B(-p), \dots, P_A(-1) P_B(-1), P_A(0) P_B(0), P_A(1) P_B(1), \dots, P_A(p) P_B(p)
$$
- For $P_C(x) = P_A(x) P_B(x)$ these products are $2p + 1$ many values of $P_C(x)$:
$$
P_C(-p) = P_A(-p) P_B(-p), \dots, P_C(0) = P_A(0) P_B(0), \dots, P_C(p) = P_A(p) P_B(p)
$$
- Let $C_0, C_1, \dots, C_{2p}$ be the coefficients of the product polynomial $C(x)$, i.e. let
$$
P_C(x) = C_{2p} x^{2p} + C_{2p - 1} x^{2p - 1} + \dots + C_0
$$
- We now have
$$
\begin{array}{l}
  C_{2p}(-p)^{2p} + C_{2p - 1}(-p)^{2p - 1} + \dots + C_0 = P_C(-p) \\
  C_{2p}(-(p - 1))^{2p} + C_{2p - 1}(-(p - 1))^{2p - 1} + \dots + C_0 = P_C(-(p - 1)) \\
  \qquad \vdots \\
  C_{2p}(p - 1)^{2p} + C_{2p - 1}(p - 1)^{2p - 1} + \dots + C_0 = P_C(p - 1) \\
  C_{2p}(p)^{2p} + C_{2p - 1}(p)^{2p - 1} + \dots + C_0 = P_C(p)
\end{array}
$$

This is just a system of linear equations, that can be solved for $C_0, C_1, \dots, C_{2p}$. We can apply the inverse Vandermonde matrix as described earlier, thus the coefficients $C_i$ can be obtained in linear time.
