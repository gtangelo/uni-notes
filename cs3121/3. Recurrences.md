# Recurrences

## Asymptotic Notation

'**Big Oh**' notation $f(n) = \mathcal{O}(g(n))$ is an abbreviation for: "there exist positive constants $c$ and $n_0$ such that $0 \le f(n) \le c g(n)$ for all $n \ge n_0$".

- In this case we say that $g(n)$ is an asymptotic upper bound for $f(n)$
- $f(n) = \mathcal{O}(g(n))$ means that $f(n)$ does not grow substantially faster than $g(n)$ because a multiple of $g(n)$ eventually dominates $f(n)$

'**Omega**' notation $f(n) = \Omega(g(n))$ is an abbreviation for: "there exists positive constants $c$ and $n_0$ such that $0 \le c g(n) \le f(n)$ for all $n \ge n_0$.

- In this case we say that $g(n)$ is an asymptotic lower bound for $f(n)$
- $f(n) = \Omega(g(n))$ essentially says that $f(n)$ grows at least as fast as $g(n)$, because $f(n)$ eventually dominates a multiple of $g(n)$
- Since $c g(n) \le f(n)$ if and only if $g(n) \le \frac{1}{c} f(n)$ we have $f(n) = \omega(g(n))$ if and only if $g(n) = \mathcal{O}(f(n))$

'**Theta**' notation $f(n) = \Theta(g(n))$ only holds if and only if $f(n) = \mathcal{O}(g(n))$ and $f(n) = \sum(g(n))$; thus, $f(n)$ and $g(n)$ have the same asymptotic growth rate.

## Recurrences

Recurrences are important to us because they arise in estimations of time complexity of divide-and-conquer algorithms.

Let $a \ge 1$ be an integer and $b > 1$ a real number

Assume that a divide-and-conquer algorithm:

- reduces a problem of size $n$ to $a$ many problems of smaller size $n / b$
- the overhead cost of splitting up/combining the solutions for size $n / b$ into a solution for size $n$ is $f(n)$

then the time complexity of such algorithm satisfies $T(n) = aT (\frac{n}{b}) + f(n)$.

Some recurrences can be solved explicitly, but this tends to be tricky. Fortunately, to estimate the efficiency of an algorithm we do not need the exact solution of a recurrence. Rather, we only need to find:

- the **growth rate** of the solution i.e. its asymptotic behaviour
- the (approximate) **sizes of the constants** involved.

This is what the **Master Theorem** provides (when it is applicable).

### Master Theorem

Let

- $a \ge 1$ be an integer and and $b > 1$ a real
- $f(n) > 0$ be a non-decreasing function
- $T(n)$ be the solution of the recurrence $T(n) = aT (\frac{n}{b}) + f(n)$

Then:

- If $f(n) = \mathcal{O}(n^{\log_b a - \epsilon})$ for some $\epsilon > 0$ then $T(n) = \Theta(n^{\log_b a})$
- If $f(n) = \Theta(n^{\log_b a})$ then $T(n) = \Theta(n^{\log_b a} \log_2 n)$
- If $f(n) = \Omega(n^{\log_b a + \epsilon})$ for some $\epsilon > 0$ **and** for some $c < 1$ and some $n_0$, $a f(\frac{n}{b}) \le c f(n)$ holds for all $n > n_0$, then $T(n) = \Theta(f(n))$

If none of these conditions hold, the Master Theorem is not applicable.

#### Examples

Let $T(n) = 4T(\frac{n}{2}) + n$, then $n^{\log_b a} = n^{\log_2 4} = n^2$, thus $f(n) = n = \mathcal{O}(n^{2 - \epsilon})$ for any $\epsilon < 1$. The condition of case (1) is satisfied and so $T(n) = \Theta(n^2)$.

Let $T(n) = 2T(\frac{n}{2}) + cn$, then $n^{\log_b a} = n^{\log_2 2} = n^1 = n$, thus $f(n) = cn = \Theta(n) = \Theta(n^{\log_2 2})$. The condition of case (2) is satisfied and so $T(n) = \Theta(n^{\log_2 2} \log n) = \Theta(n \log n)$.

Let $T(n) = 3T(\frac{n}{4}) + n$, then $n^{\log_b a} = n^{\log_4 3} < n^{0.8}$, thus $f(n) = n = \Omega(n^{0.8 + \epsilon})$ for any $\epsilon < 0.2$. Also, $af(\frac{n}{b}) = 3f(\frac{n}{4}) = \frac{3}{4} < cn = cf(n)$ for $c = 0.8 < 1$..The condition of case (3) is satisfied and so $T(n) = \Theta(f(n)) = \Theta(n)$.
