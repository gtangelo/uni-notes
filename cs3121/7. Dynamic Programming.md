# Dynamic Programming

The main idea of Dynamic Programming is to build an optimal solution to a problem from optimal solutions for (carefully chosen) smaller size subproblems.

Subproblems are chosen in a way which allows recursive construction of optimal solutions to such subproblems from optimal solutions to smaller size subproblems.

Efficiency of DP comes from the fact that that the sets of subproblems needed to solve larger problems heavily overlap; each subproblem is solved only once and its solution is stored in a table for multiple use for solving larger problems.

## Activity Selection

**Problem**: Given a list of activities $a_i, 1 \le i \le n$ with starting times $s_i$ and finishing times $f_i$ where no two activities can take place simultaneously, find a subset of compatible activities of **maximal total duration**.

**Solution**: We start by sorting these activities by their finishing time into a non-decreasing sequence, so will assume that $f_1 \le f_2 \le \dots \le f_n$.

For every $i \le n$ we solve the following subproblem $P(i)$:

- Find a subsequence $\sigma_i$ of the sequence of activities $S_i = \langle a_1, a_2, \dots, a_i \rangle$ such that:
  1. $\sigma_i$ consists of non-overlapping activities
  2. $\sigma_i$ ends with activity $a_i$
  3. $\sigma_i$  of maximal total duration among all subsequences of $S_i$ which satisfy (1) and (2)

Let $T(i)$ be the total duration of the optimal solution $S(i)$ of the subproblem $P(i)$. For $S(1)$ we choose $a_1$, thus $T(1) = f_1 - s_1$.

**Recursion**: assuming that we have solved subproblems for all $j < i$ and stored them in a table, we let

$$
T(i) = \max \{ T(j) + f_i - s_i : j < i \land f_j < s_i \}
$$

In the table, besides $T(i)$ we also store $j$ for which the above max is achieved.

We now let $T_{max} = \max \{ T(i) : i \le n \}$. 

We can now reconstruct the optimal sequence which solves our problem from the table of partial solutions, because in the $i$th slot of the table, besides $T(i)$ we also store $j$ such that the optimal solution of $P(i)$ extends the optimal solution of subproblem $P(j)$.

## Longest Increasing Subsequence

**Problem**: Given a sequence of $n$ real number $A[1..n]$, determine a subsequence (not necessarily contiguous) of maximum length in which the values in the subsequence are strictly increasing.

**Solution**: For each $i \le n$ we solve the following subproblem $P(i)$:

- Find a subsequence of the sequence $A[1..i]$ of maximum length in which the values are strictly increasing and which ends with $A[i]$.

**Recursion**: Assume we have solved all the subproblems for $j < i$. We now look for all $A[m]$ such that $m < i$ and such that $A[m] < A[i]$. Among those we pick $m$ which produced the longest increasing subsequence ending with $A[m]$ and extend it with $A[i]$ to obtain the longest increasing subsequence which ends with $A[i]$.

## Integer Knapsack Problem (Duplicate Items Allowed)

**Problem**: You have $n$ types of items, all items of kind $i$ are identical and of weight $w_i$ and value $v_i$. You also have a knapsack of capacity $C$. Choose a combination of available items which all fit in the knapsack and whose value is as large as possible. You can take any number of items of each kind.

**Solution**: DP recursion on the capacity $C$ of the knapsack. We build a table of optimal solutions for all knapsacks of capacities $i \le C$. 

Assume we have solved the problem for all knapsacks of capacities $j < i$. We now look at optimal solutions $opc(i - w_m)$ for all knapsacks of capacities $i - w_m$ for all $1 \le m \le n$. Choose the one for which $opt(i - w_m) + v_m$ is the largest.

Add to such optimal solution for the knapsack of size $i - w_m$ item $m$ to obtain a packing of a knapsack of size $i$ of the highest possible value.

Thus, $opt(i) = \max \{ opt(i - w_m) + v_m : 1 \le m \le n \}$. After $C$ many steps we obtain $opt(C)$ which is what we need.

## Integer Knapsack Problem (Duplicate Items Not Allowed)

**Problem**: You have $n$ items (some of which can be identical); item $I_i$ is of weight $w_i$ and value $v_i$. You also have a knapsack of capacity $C$. Choose a combination of available items which all fit in the knapsack and whose value is as large as possible.

**Solution**: This is an example of a "2D" recursion. We will be filling a table of size $n \times C$, row by row. Subproblems $P(i, c)$ for all $i \le n$ and $c \le C$ will be of the form: *choose from items $I_1, I_2, \dots, I_i$ a subset which fits in a knapsack of capacity $c$ and is of the largest possible total value*.

Fix now $i \le n$ and $c \le C$ and assume we have solved the subproblems for:

1. All $j < i$ and all knapsacks of capacities from 1 to $C$
2. For $i$ we have solved the problems for all capacities $d < c$

We now have two options: either we take item $I_i$ or we do now. So we look at the optimal solutions $opt(i - 1, c - w_i)$ and $opt(i - 1, c)$.

If $opt(i - 1, c - w_i) + v_i > opt(i - 1, c)$ then we set $opt(i, c) = opt(i - 1, c - w_i) + v_i$, else we set $opt(i, c) = opt(i - 1, c)$.

The final solution will be given by $opt(n, C)$.

## Bellman Ford Algorithm

**Problem**: Given a directed weighted graph $G = (V, E)$ with weights which can be negative, but without cycles of negative total weight and a vertex $s \in V$, find the shortest path from vertex $s$ to every other vertex $t$.

**Solution**: Since there are no negative weight cycles, the shortest path cannot contain cycles, because a cycle can be excised to produce a shorter path. Thus, every shortest path can have at most $\|V\| - 1$ edges.

**Subproblem**: For every $v \in V$ and every $i, (1 \le i \le n - 1)$, let $opt(i, v)$ be the length of a shortest path from $s$ to $v$ which contains at most $i$ edges.

Our goal is to find for every vertex $t \in G$ the value of $opt(n - 1, t)$ and the path which achieves such a length.

Let us denote the length of the shortest path from $s$ to $v$ among all paths which contain at most $i$ edges by $opt(i, v)$, and let $pred(i, v)$ be the immediate predecessor of vertex $v$ on such shortest path.

**Recursion**:

$$
opt(i, v) = \min(opt(i - 1, v), \min_{p \in V} \{ opt(i - 1, p) + w(e(p, v)) \})
$$

$$
pred(i, v)
=
\begin{cases}
  pred(i - 1, v) & \text{if} \ \min_{p \in V} \{ opt(i - 1, p) + w(e(p, v)) \} \ge pred(i - 1, v) \\
  \arg \min_{p \in V} \{ opt(i - 1, p) + w(e(p, v)) \} & \text{otherwise}
\end{cases}
$$

Here $w(e(p, v))$ is the weight of the edge $e(p, v)$ from vertex $p$ to vertex $v$.

This algorithm produces shortest paths from $s$ to every other vertex in the graph.

The method employed is sometimes called "relaxation", because we progressively relax the additional constraint on how many edges the shortest paths can contain.

## Floyd Warshall Algorithm

**Problem**: Let $G = (V, E)$ be a directed weighted graph where $V = \{ v_1, v_2, \dots, v_n \}$ and where weights $w(e(v_p, v_q))$ of edges $e(v_p, v_q)$ can be negative, but there are no negative weight cycles.

**Solution**: we can use a somewhat similar idea to obtain the shortest paths from every vertex $v_p$ to every vertex $v_q$ (including back to $v_p$).

Let $opt(k, v_p, v_q)$ be the length of the shortest path from a vertex $v_p$ to a vertex $v_q$ such that all intermediate vertices are among vertices $\{ v_1, v_2, \dots, v_k \}, (1 \le k \le n)$.

Then $opt(k, v_p, v_q) = \min \{ opt(k - 1, v_p, v_q), opt(k - 1, v_p, v_k) + opt(k - 1, v_k, v_q) \}$.

Thus, we gradually relax the constraint that the intermediary vertices have to belong to $\{ v_1, v_2, \dots, v_k \}$.
