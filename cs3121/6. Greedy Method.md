# Resources

- https://www.youtube.com/playlist?list=PLfFeAJ-vQopt_S5XlayyvDFL_mi2pGJE3
- https://www.youtube.com/watch?v=tKwnms5iRBU&list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp&index=16

# Greedy Algorithms

A greedy algorithm is any algorithm that follows the problem-solving heuristic of **making the locally optimal choice** at each stage.

- **Greedy choice property** - We can make whatever choice seems best at the moment and then solve the subproblems that arise later. It iteratively makes one greedy choice after another, reducing each given problem into a smaller one which means a greedy algorithm never reconsiders its choices. 
- **Optimal substructure** - if an optimal solution to the problem contains optimal solutions to the sub-problems

## Activity Selection

**Problem**: Given a list of activities $a_i$ ($1 \le a \le n$) with starting times $s_i$ and finishing times $f_i$ where no two activities can take place simultaneously, find a maximum size subset of compatible activities.

**Solution**: Among the activities which do not conflict with the previously chosen activities always chose the one with the earliest end time.

To prove optimality of this greedy solution we show that any optimal solution can be transformed into the greedy solution with equal number of activities:

- Find the first place where the chosen activity violates the greedy choice
- Show that replacing that activity with the greedy choice produces a non-conflicting selection with the same number of activities
- Continue in this manner till you "morph" your optimal solution into the greedy solution, thus proving the greedy solution is also optimal

## Minimising Job Lateness

**Problem**: We are given a start time $T_0$ and a list of jobs $a_i$ ($1 \le i \le n$), with duration times $t_i$ and deadlines $d_i$. Only one job can be performed at any time; all jobs have to be completed. If a job $a_i$ is completed at a finishing time $f_i > d_i$, then we say that it has incurred lateness $l_i = f_i - d_i$. Schedule all the jobs so that the lateness of the job with the largest lateness is minimised.

**Solution**: Ignore job durations and schedule jobs in the increasing order of deadlines.

## Tape Storage

**Problem**: We are given a list of $n$ files $f_i$ of lengths $l_i$ which have to be stored on a tape. Each file is equally likely to be needed. To retrieve a file, one must start from the beginning of the tape and scan it until the file is found and read. Order the files on the tape so that the average (expected) retrieval time is minimised.

**Solution**: If the files are stored in order $l_1, l_2, \dots, l_n$ then the expected time is proportional to

$$
l_1 + (l_1 + l_2) + (l_1 + l_2 + l_3) + \dots + (l_1 + l_2 + l_3 + \dots + l_n) = n l_1 + (n - 1) l_2 + (n - 2) l_3 + \dots + 2 l_{n - 1} + l_n
$$

This is minimised if $l_1 \le l_2 \le l_3 \dots l_n$.

## Kruskal's Algorithm

Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest.

- We order the edges $E$ in a non-decreasing order with respect to their weights
- We build a tree by adding edges, one at each step of construction
- An edge $e_i$ is added at a round $i$ of construction whenever its addition does not introduce a cycle in the graph constructed thus far
- If adding an edge does introduce a cycle, that edge is discarded
- The process terminates when the list of all edges has been exhausted

## $k$-clustering Of Maximum Spacing

**Problem**: Given a complete graph $G$ with weighted edges representing distances between the two vertices, partition the vertices of $G$ into $k$ disjoint subsets so that the minimal distance between two points belonging to different sets of the partition is as large as possible. Thus, we want a partition into $k$ disjoint sets which are as far apart as possible.

**Solution**: Sort the edges in an increasing order and start performing the usual Kruskal's algorithm for building a minimal spanning tree, but stop when you obtain $k$ connected components, rather than a single spanning tree.
